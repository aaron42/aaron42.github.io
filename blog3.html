<h5>Week 3 Blog Post</h5>
<p>This week, I have started working on the technical part of the project. My planned initial approach is using the framework Gym Retro from OpenAI. It is a well-developed framework that provides a easy way to start neural network playing retro games. After the installation, and spending some time to read through the document. The first challenge I faced was that I did not find any turn-based strategy game in the integrated games, so I have to integrate them myself. I decided to pick Fire Emblem: The Sacred Stone because I played it a long time ago. After finding the ROM fairly quickly, the first challenge of the integration is the fact that the intergration UI requires a sha-1 sum to be under the same directory to run. There is no feedback from the UI at all and the documentation did not mention the need specifically, and internet searches did not find anyone facing the same issue making it quite challenging to bugfix. After the game actually runs, I get to the start of the tutorial level where Seth and Eirika face three enemies. I save the state of the game at this point as the start of the level. After that, I need to find the relevant variables, the reward function, and the done condition to actually run the program. I decided to use the HP values of each of the five characters as the variables. Interestingly, Ceth's HP is impossible to be searched in this level because the enemies can't hit him at all, which also makes it a irrelevant value for this level. The level's win condition is actually defeating only one of the three enemies. But he does not move before the other two enemies and Ceth and Eirika can't get to him without killing the two enemies before. So I have set the HPs of the three enemies all as the reward with equal weights. I have also set Eirika's HP as a penalty because her death means a loss. The done state thus is determined by either Eirika's HP or the boss's HP reaching 0. After that is done, I decided to try out the set environment. I have tried the example reinforcement learning approaches on my integration. It turns out the default approaches do not work that well. Random Agent approach obviously only confirms that my integration is set properly and the reward actually works. The brute approach, which I have spent more time on running. The approach is trying to use brute force approach to find out the best sequence of button presses that do well in the game. Despite running it for a long time, I was not able to use this simple approach to reach the end of the level. I believe the reasoning of the failure is because in the game Fire Emblem, the attack animation is quite long, so even when the algorithm does the attack correctly, it does not get the damage feedback instantly, making it difficult for the approach to learn what is actually the sequence of the button presses that lead to the reward given. Next week, I will most likely start programing my approach for Gym Retro. The approach should aknowledge the fact that a lot of the feedback from Fire Emblem is not instant and calculate what it needs to do considering this.</p>

